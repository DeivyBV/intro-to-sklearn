{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Feature reduction, selection and creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the learning easier or better  beforehand -  feature reduction/selection/creation\n",
    "* PCA\n",
    "* SelectKBest\n",
    "* One-Hot Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal component analysis (aka PCA)\n",
    "* Reduces dimensions (number of features), based on what information explains the most variance (or signal)\n",
    "* Considered unsupervised learning\n",
    "* Useful for very large feature space (e.g. say the botanist in charge of the iris dataset measured 100 more parts of the flower and thus there were 104 columns instead of 4)\n",
    "* More about PCA on wikipedia [here](https://en.wikipedia.org/wiki/Principal_component_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PCA for dimensionality reduction\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# perform principal component analysis\n",
    "pca = decomposition.PCA(.95)\n",
    "pca.fit(X)\n",
    "X_t = pca.transform(X)\n",
    "(X_t[:, 0])\n",
    "\n",
    "# import numpy and matplotlib for plotting (and set some stuff)\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# let's separate out data based on first two principle components\n",
    "x1, x2 = X_t[:, 0], X_t[:, 1]\n",
    "\n",
    "\n",
    "# please don't worry about details of the plotting below \n",
    "#  (note: you can get the iris names below from iris.target_names, also in docs)\n",
    "c1 = np.array(list('rbg')) # colors\n",
    "colors = c1[y] # y coded by color\n",
    "classes = iris.target_names[y] # y coded by iris name\n",
    "for (i, cla) in enumerate(set(classes)):\n",
    "    xc = [p for (j, p) in enumerate(x1) if classes[j] == cla]\n",
    "    yc = [p for (j, p) in enumerate(x2) if classes[j] == cla]\n",
    "    cols = [c for (j, c) in enumerate(colors) if classes[j] == cla]\n",
    "    plt.scatter(xc, yc, c = cols, label = cla)\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.xlabel('Principal Component 1')\n",
    "plt.legend(loc = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting k top scoring features (also dimensionality reduction)\n",
    "* Considered unsupervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SelectKBest for selecting top-scoring features\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "# Do feature selection\n",
    "#  input is scoring function (here chi2) to get univariate p-values\n",
    "#  and number of top-scoring features (k) - here we get the top 2\n",
    "X_t = SelectKBest(chi2, k = 2).fit_transform(X, y)\n",
    "\n",
    "print(X_t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note on scoring function selection in `SelectKBest` tranformations:</b>\n",
    "* For regression - f_regression\n",
    "* For classification - chi2, f_classif\n",
    "\n",
    "### One Hot Encoding\n",
    "* It's an operation on feature labels - a method of dummying variable\n",
    "* Expands the feature space by nature of transform - later this can be processed further with a dimensionality reduction (the dummied variables are now their own features)\n",
    "* FYI:  One hot encoding variables is needed for python ML module `tenorflow`\n",
    "* The code cell below should help make this clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# OneHotEncoder for dummying variables\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame({'index': range(1, 7),\n",
    "                    'state': ['WA', 'NY', 'CO', 'NY', 'CA', 'WA']})\n",
    "print(data)\n",
    "\n",
    "# We encode both our categorical variable and it's labels\n",
    "enc = OneHotEncoder()\n",
    "label_enc = LabelEncoder() # remember the labels here\n",
    "\n",
    "# Encode labels (can use for discrete numerical values as well)\n",
    "data_label_encoded = label_enc.fit_transform(data['state'])\n",
    "data['state'] = data_label_encoded\n",
    "\n",
    "# Encode and \"dummy\" variables\n",
    "data_feature_one_hot_encoded = enc.fit_transform(data[['state']])\n",
    "\n",
    "# Put into dataframe to look nicer and decode state dummy variables to original state values\n",
    "# TRY:  compare the original input data (look at row numbers) to one hot encoding results\n",
    "#   --> do they match??\n",
    "pd.DataFrame(data_feature_one_hot_encoded.toarray(), columns = label_enc.inverse_transform(range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Encoded labels as dummy variables\n",
    "print(data_label_encoded)\n",
    "\n",
    "# Decoded\n",
    "print(label_enc.inverse_transform(data_label_encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXERCISE:  Use one hot encoding to \"recode\" the iris data's extra suprise column (we are going to add a categorical variable here to play with...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "a = pd.DataFrame(X, \n",
    "                columns = ['Sepal length', 'Sepal width', 'Petal length', 'Petal width'])\n",
    "\n",
    "col5 = pd.DataFrame(np.random.randint(1, 4, size = len(y)))\n",
    "\n",
    "X_plus = pd.concat([a, col5], axis = 1)\n",
    "X_plus.head(20)\n",
    "\n",
    "# ...now one-hot-encode..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by a Microsoft Employee.\n",
    "\t\n",
    "The MIT License (MIT)<br>\n",
    "Copyright (c) 2016 Micheleen Harris"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
