{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize and Explore\n",
    "## The Dataset - Fisher's Irises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most machine learning algorithms implemented in scikit-learn expect data to be stored in a\n",
    "**two-dimensional array or matrix**.  The arrays can be\n",
    "either ``numpy`` arrays, or in some cases ``scipy.sparse`` matrices.\n",
    "The size of the array is expected to be `n_samples x n_features`.\n",
    "\n",
    "- **n_samples:**   The number of samples: each sample is an item to process (e.g. classify).\n",
    "  A sample can be a document, a picture, a sound, a video, an astronomical object,\n",
    "  a row in database or CSV file,\n",
    "  or whatever you can describe with a fixed set of quantitative traits.\n",
    "- **n_features:**  The number of features or distinct traits that can be used to describe each\n",
    "  item in a quantitative manner.  Features are generally real-valued, but may be boolean or\n",
    "  discrete-valued in some cases.<br><br>\n",
    "<font color = \"lightgrey\">\n",
    "The number of features must be fixed in advance. However it can be very high dimensional\n",
    "(e.g. millions of features) with most of them being zeros for a given sample. This is a case\n",
    "where `scipy.sparse` matrices can be useful, in that they are\n",
    "much more memory-efficient than numpy arrays.\n",
    "</font><br><br>\n",
    "If there are labels or targets, they need to be stored in **one-dimensional arrays or lists**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we are going to use the <b>`iris`</b> dataset which comes with `sklearn`.  It's fairly small as we'll see shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>Remember our ML TIP:  Ask sharp questions.</b><br>e.g. What type of flower is this (pictured below) closest to of the three given classes?\n",
    "\n",
    "(This links out to source)\n",
    "<a href=\"http://www.madlantern.com/photography/wild-iris/\"><img border=\"0\" alt=\"iris species\" src=\"imgs/iris-setosa.jpg\" width=\"400\" height=\"400\"></a>\n",
    "\n",
    "### Labels (species names/classes):\n",
    "(This links out to source)\n",
    "<a href=\"http://articles.concreteinteractive.com/machine-learning-a-new-tool-for-humanity/\"><img border=\"0\" alt=\"iris species\" src=\"imgs/irises.png\" width=\"500\" height=\"500\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"lightgrey\"> TIP: Commonly, machine learning algorithms will require your data to be standardized, normalized or even reguarlized and preprocessed.  In `sklearn` the data must also take on a certain structure as discussed above.</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUICK QUESTION:\n",
    "1.  What do you expect this data set to be if you are trying to recognize an iris species?\n",
    "* For our `[n_samples x n_features]` data array, what do you think\n",
    "    * the samples are?\n",
    "    * the features are?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "print(type(iris.data))\n",
    "print(type(iris.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Dive In!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features (aka columns in data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "pd.DataFrame({'feature name': iris.feature_names})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Targets (aka labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "pd.DataFrame({'target name': iris.target_names})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `sklearn` TIP: all included datasets for have at least `feature_names` and sometimes `target_names`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get to know the data - visualize and explore\n",
    "* Features (columns/measurements) come from this diagram (links out to source on kaggle):\n",
    "<a href=\"http://blog.kaggle.com/2015/04/22/scikit-learn-video-3-machine-learning-first-steps-with-the-iris-dataset/\"><img border=\"0\" alt=\"iris data features\" src=\"imgs/iris_petal_sepal.png\" width=\"200\" height=\"200\"></a>\n",
    "* Shape\n",
    "* Peek at data\n",
    "* Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Shape and representation<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# How many data points (rows) x how many features (columns)\n",
    "print(iris.data.shape)\n",
    "print(iris.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Sneak a peek at data (a reminder of your `pandas` dataframe methods)<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert to pandas df (adding real column names)\n",
    "iris.df = pd.DataFrame(iris.data, \n",
    "                       columns = iris.feature_names)\n",
    "\n",
    "\n",
    "# first few rows\n",
    "iris.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Describe the dataset with some summary statitsics<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# summary stats\n",
    "iris.df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We don't have to do much with the `iris` dataset.  It has no missing values.  It's already in numpy arrays and has the correct shape for `sklearn`.  However we could try <b>standardization</b> and/or <b>normalization</b>. (later, in the transforms section, we will show one hot encoding, a preprocessing step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing (Bonus Material)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p>What you might have to do before using a learner in `sklearn`:</p>\n",
    "1. Non-numerics transformed to numeric (tip: use applymap() method from `pandas`)\n",
    "* Fill in missing values\n",
    "* Standardization\n",
    "* Normalization\n",
    "* Encoding categorical features (e.g. one-hot encoding or dummy variables)\n",
    "\n",
    "<b>Features should end up in a numpy.ndarray (hence numeric) and labels in a list.</b>\n",
    "\n",
    "Data options:\n",
    "* Use pre-processed [datasets](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets) from scikit-learn\n",
    "* [Create your own](http://scikit-learn.org/stable/datasets/index.html#sample-generators)\n",
    "* Read from a file\n",
    "\n",
    "If you use your own data or \"real-world\" data you will likely have to do some data wrangling and need to leverage `pandas` for some data manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardization - make our data look like a standard Gaussian distribution (commonly needed for `sklearn` learners)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> FYI: you'll commonly see the data or feature set (ML word for data without it's labels) represented as a capital <b>X</b> and the targets or labels (if we have them) represented as a lowercase <b>y</b>.  This is because the data is a 2D array or list of lists and the targets are a 1D array or simple list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Standardization aka scaling\n",
    "from sklearn import preprocessing, datasets\n",
    "\n",
    "# make sure we have iris loaded\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# scale it to a gaussian distribution\n",
    "X_scaled = preprocessing.scale(X)\n",
    "\n",
    "# how does it look now\n",
    "pd.DataFrame(X_scaled).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's just confirm our standardization worked (mean is 0 w/ unit variance)\n",
    "pd.DataFrame(X_scaled).describe()\n",
    "\n",
    "# also could:\n",
    "#print(X_scaled.mean(axis = 0))\n",
    "#print(X_scaled.std(axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> PRO TIP: To save our standardization and reapply later (say to the test set or some new data), create a transformer object like so:\n",
    "```python\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "# apply to a new dataset (e.g. test set):\n",
    "scaler.transform(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization - scaling samples <i>individually</i> to have unit norm\n",
    "* This type of scaling is really important if doing some downstream transformations and learning (see sklearn docs [here](http://scikit-learn.org/stable/modules/preprocessing.html#normalization) for more) where similarity of pairs of samples is examined\n",
    "* A basic intro to normalization and the unit vector can be found [here](http://freetext.org/Introduction_to_Linear_Algebra/Basic_Vector_Operations/Normalization/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Standardization aka scaling\n",
    "from sklearn import preprocessing, datasets\n",
    "\n",
    "# make sure we have iris loaded\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# scale it to a gaussian distribution\n",
    "X_norm = preprocessing.normalize(X, norm='l1')\n",
    "\n",
    "# how does it look now\n",
    "pd.DataFrame(X_norm).tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's just confirm our standardization worked (mean is 0 w/ unit variance)\n",
    "pd.DataFrame(X_norm).describe()\n",
    "\n",
    "# cumulative sum of normalized and original data:\n",
    "#print(pd.DataFrame(X_norm.cumsum().reshape(X.shape)).tail())\n",
    "#print(pd.DataFrame(X).cumsum().tail())\n",
    "\n",
    "# unit norm (convert to unit vectors) - all row sums should be 1 now\n",
    "X_norm.sum(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> PRO TIP: To save our normalization (like standardization above) and reapply later (say to the test set or some new data), create a transformer object like so:\n",
    "```python\n",
    "normalizer = preprocessing.Normalizer().fit(X_train)\n",
    "# apply to a new dataset (e.g. test set):\n",
    "normalizer.transform(X_test) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by a Microsoft Employee.\n",
    "\t\n",
    "The MIT License (MIT)<br>\n",
    "Copyright (c) 2016 Micheleen Harris"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
