{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with `scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning 101\n",
    "\n",
    "It's said in different ways, but I like the way Jake VanderPlas defines ML:\n",
    "\n",
    "> Machine Learning is about building <b>programs with tunable parameters</b> (typically an array of floating point values) that are adjusted automatically so as to improve their behavior by <b>adapting to previously seen data</b>.\n",
    "\n",
    "He goes on to say:\n",
    "\n",
    "> Machine Learning can be considered a <b>subfield of Artificial Intelligence</b> since those algorithms can be seen as building blocks to make computers learn to behave more intelligently by somehow <b>generalizing</b> rather that just storing and retrieving data items like a database system would do.\n",
    "\n",
    "(more [here](http://www.astroml.org/sklearn_tutorial/general_concepts.html))\n",
    "\n",
    "ML is much more than writing a program.  ML experts write clever and robust algorithms which can generalize to answer different, but specific questions.  There are still types of questions that a certain algorithm can not or should not be used to answer.  I say answer instead of solve, because even with an answer one should evaluate whether it is a good answer or bad answer.  Also, just an in statistics, one needs to be careful about assumptions and limitations of an algorithm and the subsequent model that is built from it.  Here's my hand-drawn diagram of the machine learning process.<br>\n",
    "\n",
    "<img src='imgs/ml_process.png' alt=\"Smiley face\" width=\"550\">\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "Below, we are going to show a simple case of <i>classification</i>.  In the figure we show a collection of 2D data, colored by their class labels (imagine one class is labeled \"red\" and the other \"blue\").  \n",
    "\n",
    "> The `fig_code` module is credited to Jake VanderPlas and was cloned from his github repo [here](https://github.com/jakevdp/sklearn_pycon2015) - also on our repo is his license file since he asked us to include that if we use his source code. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot settings for notebook\n",
    "\n",
    "# so that plots show up in notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# seaborn here is used for aesthetics.\n",
    "# here, setting seaborn plot defaults (this can be safely commented out)\n",
    "import seaborn; seaborn.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import an example plot from the figures directory\n",
    "from fig_code import plot_sgd_separator\n",
    "plot_sgd_separator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the vector which best separates the two classes, \"red\" and \"blue\" using a classification algorithm called Stochastic Gradient Decent (don't worry about the detail yet).  The confidence intervals are shown as dashed lines.\n",
    "\n",
    "This demonstrates a very important aspect of ML and that is the algorithm is <i>generalizable</i>, i.e., if we add some new data, a new point, the algorithm can <i>predict</i> whether is should be in the \"red\" or \"blue\" category.\n",
    "\n",
    "Here are some details of the code used above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "import numpy as np\n",
    "\n",
    "# we create 50 separable points\n",
    "X, y = make_blobs(n_samples=50, centers=2,\n",
    "                      random_state=0, cluster_std=0.60)\n",
    "\n",
    "\n",
    "# what's in X and what's in y??\n",
    "print(X[0:10,:])\n",
    "print(y[0:10])\n",
    "\n",
    "target_names = np.array(['red', 'blue']) # <-- what am I naming you think?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fit the model -> more details later\n",
    "clf = SGDClassifier(loss=\"hinge\", alpha=0.01,\n",
    "                    n_iter=200, fit_intercept=True)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add some of your own data and make a prediction in the cell below.\n",
    "\n",
    "Data could be a single x, y point or array of x, y points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test = [] # <-- your data here\n",
    "y_pred = clf.predict(___) # <-- what goes here?\n",
    "\n",
    "# predictions\n",
    "target_names[y_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> <b>ML TIP:  ML can only answer 5 questions:</b>\n",
    "* How much/how many?\n",
    "* Which category?\n",
    "* Which group?\n",
    "* Is it weird?\n",
    "* Which action?\n",
    "\n",
    "As far as algorithms for learning a model (i.e. running some training data through an algorithm), it's nice to think of them in two different ways (with the help of the [machine learning wikipedia article](https://en.wikipedia.org/wiki/Machine_learning)).  The first way of thinking about ML, is by the type of information or <i>input</i> given to a system.  So, given that criteria there are three classical categories:\n",
    "1.  Supervised learning - we get the data and the labels\n",
    "2.  Unsupervised learning - only get the data (no labels)\n",
    "3.  Reinforcement learning - reward/penalty based information (feedback)\n",
    "\n",
    "Another way of categorizing ML approaches, is to think of the desired <i>output</i>:\n",
    "1.  Classification\n",
    "2.  Regression\n",
    "3.  Clustering\n",
    "4.  Density estimation\n",
    "5.  Dimensionality reduction\n",
    "\n",
    "--> This second approach (by desired <i>output</i>) is how `sklearn` categorizes it's ML algorithms.\n",
    "\n",
    "\n",
    "#### The problem solved in supervised learning (e.g. classification, regression)\n",
    "\n",
    "Supervised learning consists in learning the link between two datasets: the observed data X and an external variable y that we are trying to predict, usually called “target” or “labels”. Most often, y is a 1D array of length n_samples.\n",
    "\n",
    "All supervised estimators in `sklearn` implement a `fit(X, y)` method to fit the model and a `predict(X)` method that, given unlabeled observations X, returns the predicted labels y.\n",
    "\n",
    "Common algorithms you will use to train a model and then use trying to predict the labels of unknown observations are: <b>classification</b> and <b>regression</b>.  There are many types of classification and regression (for examples check out the `sklearn` algorithm cheatsheet below).\n",
    "\n",
    "#### The problem solved in <i>un</i>supervised learning \n",
    "In machine learning, the problem of unsupervised learning is that of trying to find <b>hidden structure</b> in unlabeled data.\n",
    "\n",
    "Unsupervised models have a `fit()`, `transform()` and/or `fit_transform()` in `sklearn`.\n",
    "\n",
    "#### There are some instances where ML is just not needed or appropriate for solving a problem.\n",
    "Some examples are pattern matching (e.g. regex), group-by and data mining in general (discovery vs. prediction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXERCISE: Should I use ML or can I get away with something else?\n",
    "\n",
    "Looking back at previous years, by what percent did housing prices increase over each decade?<br>\n",
    "Looking back at previous years, and given the relationship between housing prices and mean income in my area, given my income how much will a house be in two years in my area?<br>\n",
    "A vacuum like roomba has to make a decision to vacuum the living room again or return to its base.<br>\n",
    "Is this image a cat or dog?<br>\n",
    "Are orange tabby cats more common than other breeds in Austin, Texas?<br>\n",
    "Using my SQL database on housing prices, group my housing prices by whether or not the house is under 10 miles from a school.<br>\n",
    "What is the weather going to be like tomorrow?<br>\n",
    "What is the purpose of life?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A very brief introduction to scikit-learn (aka `sklearn`)\n",
    "\n",
    "This module is not meant to be a comprehensive introduction to ML, but rather an introduction to the current de facto tool for ML in python.  As a gentle intro, it is helpful to think of the `sklearn` approach having layers of abstraction.  This famous quote certainly applies:\n",
    "\n",
    "> Easy reading is damn hard writing, and vice versa. <br>\n",
    "--Nathaniel Hawthorne\n",
    "\n",
    "In `sklearn`, you'll find you have a common programming choice: to do things very explicitly, e.g. pre-process data one step at a time, perhaps do a transformation like PCA, split data into traning and test sets, define a classifier or learner with desired parameterss, train the classifier, use the classifier to predict on a test set and then analyze how good it did.  \n",
    "\n",
    "A different approach and something `sklearn` offers is to combine some or all of the steps above into a pipeline so to speak.  For instance, one could define a pipeline which does all of these steps at one time and perhaps even pits mutlple learners against one another or does some parameter tuning with a grid search (examples will be shown towards the end).  This is what is meant here by layers of abstraction.\n",
    "\n",
    "So, in this particular module, for the most part, we will try to be explicit regarding our process and give some useful tips on options for a more automated or pipelined approach.  Just note, once you've mastered the explicit approaches you might want to explore `sklearn`'s `GridSearchCV` and `Pipeline` classes.\n",
    "\n",
    "Here is `sklearn`'s algorithm diagram - (note, this is not an exhaustive list of model options offered in `sklearn`, but serves as a good algorithm guide).\n",
    "![](imgs/ml_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your first model - a multiclass logistic regression on the iris dataset\n",
    "* `sklearn` comes with this dataset ready-to-go for `sklearn`'s algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "# Leave one value out from training set - that will be test later on\n",
    "X_train, y_train = iris.data[:-1,:], iris.target[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# our model - a multiclass regression\n",
    "logistic = LogisticRegression()\n",
    "\n",
    "# train on iris training set\n",
    "logistic.fit(X_train, y_train)\n",
    "\n",
    "X_test = iris.data[-1,:].reshape(1, -1)\n",
    "\n",
    "y_predict = logistic.predict(X_test)\n",
    "\n",
    "print('Predicted class %s, real class %s' % (\n",
    " y_predict, iris.target[-1]))\n",
    "\n",
    "print('Probabilities of membership in each class: %s' % \n",
    "      logistic.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTION:\n",
    "* What would have been good to do before plunging right in to a logistic regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some terms you will encouter as a Machine Learnest\n",
    "\n",
    "Term  | Definition\n",
    "------------- | -------------\n",
    "Training set  | set of data used to learn a model\n",
    "Test set  | set of data used to test a model\n",
    "Feature  | a variable (continuous, discrete, categorical, etc.) aka column\n",
    "Target  | Label (associated with dependent variable, what we predict)\n",
    "Learner  | Model or algorithm\n",
    "Fit, Train | learn a model with an ML algorithm using a training set\n",
    "Predict | w/ supervised learning, give a label to an unknown datum(data), w/ unsupervised decide if new data is weird, in which group, or what to do next with the new data\n",
    "Accuracy | percentage of correct predictions ((TP + TN) / total)\n",
    "Precision | percentage of correct positive predictions (TP / (FP + TP))\n",
    "Recall | percentage of positive cases caught (TP / (FN + TP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> PRO TIP: Are you a statitician?  Want to talk like a machine learning expert?  Here you go (from the friendly people at SAS ([here](http://www.sas.com/it_it/insights/analytics/machine-learning.html))): \n",
    "\n",
    "A Statistician Would Say  | A Machine Learnest Would Say\n",
    "------------- | -------------\n",
    "dependent variable  | target\n",
    "variable  | feature\n",
    "transformation  | feature creation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
