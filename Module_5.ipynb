{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Algorithms - Unsupervised Learning\n",
    "\n",
    ">  Reminder:  In machine learning, the problem of unsupervised learning is that of trying to find hidden structure in unlabeled data. Since the training set given to the learner is unlabeled, there is no error or reward signal to evaluate a potential solution. Basically, we are just finding a way to represent the data and get as much information from it that we can.\n",
    "\n",
    "HEY!  Remember PCA from above?  PCA is actually considered unsupervised learning.  We just put it up there because it's a good way to visualize data at the beginning of the ML process.\n",
    "\n",
    "Let's revisit it in a little more detail using the `iris` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn; seaborn.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA revisited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "# subset data to have only sepal width and petal length (cm) for simplification\n",
    "X = iris.data[:, 1:3]\n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "pca.fit(X)\n",
    "\n",
    "print(\"% of variance attributed to components: \"+ \\\n",
    "      ', '.join(['%.2f' % (x * 100) for x in pca.explained_variance_ratio_]))\n",
    "print('\\ncomponents and amount of variance explained by each feature:', pca.components_)\n",
    "print(pca.mean_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pca.explained_variance_` is like the magnitude of a components influence (amount of variance explained) and the `pca.components_` is like the direction vector for each feature in each component (directions of max variance in a feature in a component vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(X[:, 0], X[:, 1], 'o', alpha=0.5)\n",
    "means = pca.mean_\n",
    "\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    print(vector)\n",
    "    print(3*np.sqrt(length))\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    print(v)\n",
    "    plt.plot([means[0], v[0]+means[0]], [means[1], v[1]+means[1]], '-k', lw=3)\n",
    "\n",
    "# axis dims\n",
    "plt.xlim(0, max(X[:, 0])+3)\n",
    "plt.ylim(0, max(X[:, 1])+3)\n",
    "\n",
    "# labels\n",
    "plt.xlabel(iris.feature_names[1])\n",
    "plt.ylabel(iris.feature_names[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall, in the ML 101 module: unsupervised models have a `fit()`, `transform()` and/or `fit_transform()` in `sklearn`.\n",
    "\n",
    "\n",
    "If you want to both get a fit and new dataset with reduced dimensionality, which would you use below? (Fill in blank in code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get back to our 4D dataset\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "pca = PCA(n_components = 0.95) # keep 95% of variance\n",
    "X_trans = pca.___(X) # <- fill in the blank\n",
    "print(X.shape)\n",
    "print(X_trans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_trans[:, 0], X_trans[:, 1], c=iris.target, edgecolor='none', alpha=0.5,\n",
    "           cmap=plt.cm.get_cmap('spring', 10))\n",
    "plt.ylabel('Component 2')\n",
    "plt.xlabel('Component 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "KMeans finds cluster centers that are the mean of the points within them.  Likewise, a point is in a cluster because the cluster center is the closest cluster center for that point.\n",
    "\n",
    "KMeans employ the <i>Expectation-Maximization</i> algorithm which works as follows: \n",
    "\n",
    "1. Guess cluster centers\n",
    "* Assign points to nearest cluster\n",
    "* Set cluster centers to the mean of points\n",
    "* Repeat 1-3 until converged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "pca = PCA(n_components = 0.95) # keep 95% of variance\n",
    "X = pca.fit_transform(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fig_code import plot_kmeans_interactive\n",
    "plot_kmeans_interactive(use_iris = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cluster, datasets\n",
    "\n",
    "# data\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "k_means = cluster.KMeans(n_clusters=3)\n",
    "k_means.fit(X)\n",
    "\n",
    "# how do our original labels fit into the clusters we found?\n",
    "print(k_means.labels_[::10])\n",
    "print(y[::10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXERCISE IDEA:  Iterate over different number of clusters, n_clusters param, in Kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Novelty detection aka anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# use seaborn plotting defaults\n",
    "#import seaborn as sns; sns.set()\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(2, 10, 500), np.linspace(0, 5, 500))\n",
    "\n",
    "# Iris data\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X = X[:, 0:2]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 3)\n",
    "\n",
    "# make some outliers\n",
    "outlier_idxs = list(np.random.randint(0, len(X), 10))\n",
    "X_outliers = X.take(outlier_idxs, axis = 0)\n",
    "\n",
    "# fit the model\n",
    "clf = svm.OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.1)\n",
    "clf.fit(X_train)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred_test = clf.predict(X_test)\n",
    "y_pred_outliers = clf.predict(X_outliers)\n",
    "n_error_train = y_pred_train[y_pred_train == -1].size\n",
    "n_error_test = y_pred_test[y_pred_test == -1].size\n",
    "n_error_outliers = y_pred_outliers[y_pred_outliers == 1].size\n",
    "\n",
    "# plot the line, the points, and the nearest vectors to the plane\n",
    "Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.title(\"Novelty Detection\")\n",
    "plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.Blues_r)\n",
    "a = plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='red')\n",
    "plt.contourf(xx, yy, Z, levels=[0, Z.max()], colors='orange')\n",
    "\n",
    "b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c='white')\n",
    "b2 = plt.scatter(X_test[:, 0], X_test[:, 1], c='green')\n",
    "c = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c='red')\n",
    "plt.axis('tight')\n",
    "plt.xlim((2, 10))\n",
    "plt.ylim((0, 5))\n",
    "plt.legend([a.collections[0], b1, b2, c],\n",
    "           [\"learned frontier\", \"training observations\",\n",
    "            \"new regular observations\", \"new abnormal observations\"],\n",
    "           loc=\"upper left\",\n",
    "           prop=matplotlib.font_manager.FontProperties(size=11))\n",
    "plt.xlabel(\n",
    "    \"error train: %d/200 ; errors novel regular: %d/40 ; \"\n",
    "    \"errors novel abnormal: %d/40\"\n",
    "    % (n_error_train, n_error_test, n_error_outliers))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
