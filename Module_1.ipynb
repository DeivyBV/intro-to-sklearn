{"nbformat": 4, "cells": [{"source": ["# Machine Learning with `scikit-learn`"], "cell_type": "markdown", "metadata": {}}, {"source": ["## <img src='imgs/robotguy.png' alt=\"Smiley face\" width=\"42\" height=\"42\" align=\"left\">Learning Objectives\n", "* * *\n", "* Gain some high level knowledge around machine learning with a gentle/brief introduction\n", "* Learn the importance of pre-processing data and how `scikit-learn` expects data\n", "* See data transformations for machine learning in action\n", "* Get an idea of your options for learning on training sets and applying model for prediction\n", "* See what sort of metrics are commonly used in scikit-learn\n", "* Learn options for model evaluation\n", "* Become familiar with ways to make this process robust and simplified (pipelining and tuning parameters)\n", "\n", "### For workshop (reorg)\n", "\n", "Sections:\n", "1. ML 101 w/ code examples (incl. taste of sklearn w/ logistic regression + accuracy scores and user entered sepal and petal measurements); intro to sklearn's approach\n", "2. Our data: iris\n", "3. Supervised\n", "4. Unsupervised\n", "* Evaluating a model\n", "* What next (GridSearch & Pipeline)\n", "\n", "Flow:\n", "* a learner and visual (logistic regression, pairplot, accuracy, give some sepal and petal measurements)\n", "  * what category of ML is logistic regression? (regression or classification?)\n", "  * QUESTION: what should we have done first? Any ideas?  (EDA, visualize, pre-process if need be)\n", "* flower pics\n", "* peek at data\n", "* preprocessing from sklearn\n", "* Supervised - decision tree in detail, random forest\n", "* Unsupervised - novelty detection aka anomoly detection (note that PCA & dimensionality reduction is unsupervised)\n", "* Evaluate - metrics\n", "* what do you do with this model?  what next?\n", "* note: parameter tuning can be automated with GridSearch\n", "* note: can test many algorithms at once with Pipeline"], "cell_type": "markdown", "metadata": {}}, {"source": ["### some questions inline\n", "Does this parameter when increased cause overfitting or underfitting?  what are the implications of those cases?<br><br>\n", "Is it better to have too many false positives or too many false negatives?<br><br>\n", "What is the diffrence between outlier detection and anomaly detection?"], "cell_type": "markdown", "metadata": {}}, {"source": ["## Machine Learning 101\n", "\n", "It's said in different ways, but I like the way Jake VanderPlas defines ML:\n", "\n", "> Machine Learning is about building <b>programs with tunable parameters</b> (typically an array of floating point values) that are adjusted automatically so as to improve their behavior by <b>adapting to previously seen data</b>.\n", "\n", "He goes on to say:\n", "\n", "> Machine Learning can be considered a <b>subfield of Artificial Intelligence</b> since those algorithms can be seen as building blocks to make computers learn to behave more intelligently by somehow <b>generalizing</b> rather that just storing and retrieving data items like a database system would do.\n", "\n", "(more [here](http://www.astroml.org/sklearn_tutorial/general_concepts.html))\n", "\n", "ML is much more than writing a program.  ML experts write clever and robust algorithms which can generalize to answer different, but specific questions.  There are still types of questions that a certain algorithm can not or should not be used to answer.  I say answer instead of solve, because even with an answer one should evaluate whether it is a good answer or bad answer.  Also, just an in statistics, one needs to be careful about assumptions and limitations of an algorithm and the subsequent model that is built from it.  Here's my hand-drawn diagram of the machine learning process.<br>\n", "\n", "<img src='imgs/ml_process.png' alt=\"Smiley face\" width=\"550\">\n", "<br><br>\n"], "cell_type": "markdown", "metadata": {}}, {"source": ["### Examples\n", "Below, we are going to show a simple case of <i>classification</i>.  In the figure we show a collection of 2D data, colored by their class labels (imagine one class is labeled \"red\" and the other \"blue\").  \n", "\n", "> The `fig_code` module is credited to Jake VanderPlas and was cloned from his github repo [here](https://github.com/jakevdp/sklearn_pycon2015) - also on our repo is his license file since he asked us to include that if we use his source code. :)"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# Plot settings for notebook\n", "\n", "# so that plots show up in notebook\n", "%matplotlib inline\n", "\n", "# seaborn here is used for aesthetics.\n", "# here, setting seaborn plot defaults (this can be safely commented out)\n", "import seaborn; seaborn.set()"], "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": ["# Import an example plot from the figures directory\n", "from fig_code import plot_sgd_separator\n", "plot_sgd_separator()"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["Above is the vector which best separates the two classes, \"red\" and \"blue\" using a classification algorithm called Stochastic Gradient Decent (don't worry about the detail yet).  The confidence intervals are shown as dashed lines. - FACT CHECK CI LINE COMMENT PLEASE\n", "\n", "This demonstrates a very important aspect of ML and that is the algorithm is <i>generalizable</i>, i.e., if we add some new data, a new point, the algorithm can <i>predict</i> whether is should be in the \"red\" or \"blue\" category."], "cell_type": "markdown", "metadata": {}}, {"source": ["\n", "> <b>ML TIP:  ML can only answer 5 questions:</b>\n", "* How much/how many?\n", "* Which category?\n", "* Which group?\n", "* Is it weird?\n", "* Which action?\n", "\n", "As far as algorithms for learning a model (i.e. running some training data through an algorithm), it's nice to think of them in two different ways (with the help of the [machine learning wikipedia article](https://en.wikipedia.org/wiki/Machine_learning)).  The first way of thinking about ML, is by the type of information or <i>input</i> given to a system.  So, given that criteria there are three classical categories:\n", "1.  Supervised learning - we get the data and the labels\n", "2.  Unsupervised learning - only get the data (no labels)\n", "3.  Reinforcement learning - reward/penalty based information (feedback)\n", "\n", "Another way of categorizing ML approaches, is to think of the desired <i>output</i>:\n", "1.  Classification\n", "2.  Regression\n", "3.  Clustering\n", "4.  Density estimation\n", "5.  Dimensionality reduction\n", "\n", "--> This second approach (by desired <i>output</i>) is how `sklearn` categorizes it's ML algorithms.\n", "\n", "\n", "#### The problem solved in supervised learning (e.g. classification, regression)\n", "\n", "Supervised learning consists in learning the link between two datasets: the observed data X and an external variable y that we are trying to predict, usually called \u201ctarget\u201d or \u201clabels\u201d. Most often, y is a 1D array of length n_samples.\n", "\n", "All supervised estimators in `sklearn` implement a `fit(X, y)` method to fit the model and a `predict(X)` method that, given unlabeled observations X, returns the predicted labels y.\n", "\n", "Common algorithms you will use to train a model and then use trying to predict the labels of unknown observations are: <b>classification</b> and <b>regression</b>.  There are many types of classification and regression (for examples check out the `sklearn` algorithm cheatsheet below).\n", "\n", "#### The problem solved in <i>un</i>supervised learning \n", "In machine learning, the problem of unsupervised learning is that of trying to find <b>hidden structure</b> in unlabeled data.\n", "\n", "Unsupervised models have a `fit()`, `transform()` and/or `fit_transform()` in `sklearn`.\n", "\n", "#### There are some instances where ML is just not needed or appropriate for solving a problem.\n", "Some examples are pattern matching (e.g. regex), group-by and data mining in general (discovery vs. prediction)."], "cell_type": "markdown", "metadata": {}}, {"source": ["#### EXERCISE: Should I use ML or can I get away with something else?\n", "\n", "Looking back at previous years, by what percent did housing prices increase over each decade?<br>\n", "Looking back at previous years, and given the relationship between housing prices and mean income in my area, given my income how much will a house be in two years in my area?<br>\n", "A vacuum like roomba has to make a decision to vacuum the living room again or return to its base.<br>\n", "Is this image a cat or dog?<br>\n", "Are orange tabby cats more common than other breeds in Austin, Texas?<br>\n", "Using my SQL database on housing prices, group my housing prices by whether or not the house is under 10 miles from a school.<br>\n", "What is the weather going to be like tomorrow?<br>\n", "What is the purpose of life?"], "cell_type": "markdown", "metadata": {}}, {"source": [], "cell_type": "markdown", "metadata": {}}, {"source": ["## A very brief introduction to scikit-learn (aka `sklearn`)\n", "\n", "This module is not meant to be a comprehensive introduction to ML, but rather an introduction to the current de facto tool for ML in python.  As a gentle intro, it is helpful to think of the `sklearn` approach having layers of abstraction.  This famous quote certainly applies:\n", "\n", "> Easy reading is damn hard writing, and vice versa. <br>\n", "--Nathaniel Hawthorne\n", "\n", "In `sklearn`, you'll find you have a common programming choice: to do things very explicitly, e.g. pre-process data one step at a time, perhaps do a transformation like PCA, split data into traning and test sets, define a classifier or learner with desired parameterss, train the classifier, use the classifier to predict on a test set and then analyze how good it did.  \n", "\n", "A different approach and something `sklearn` offers is to combine some or all of the steps above into a pipeline so to speak.  For instance, one could define a pipeline which does all of these steps at one time and perhaps even pits mutlple learners against one another or does some parameter tuning with a grid search (examples will be shown towards the end).  This is what is meant here by layers of abstraction.\n", "\n", "So, in this particular module, for the most part, we will try to be explicit regarding our process and give some useful tips on options for a more automated or pipelined approach.  Just note, once you've mastered the explicit approaches you might want to explore `sklearn`'s `GridSearchCV` and `Pipeline` classes.\n", "\n", "Here is `sklearn`'s algorithm diagram - (note, this is not an exhaustive list of model options offered in `sklearn`, but serves as a good algorithm guide).\n", "![](imgs/ml_map.png)"], "cell_type": "markdown", "metadata": {}}, {"source": ["### Your first model - a multiclass logistic regression on the iris dataset\n", "* `sklearn` comes with this dataset ready-to-go for `sklearn`'s algorithms"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["from sklearn.datasets import load_iris\n", "iris = load_iris()\n", "\n", "# Leave one value out from training set - that will be test later on\n", "X_train, y_train = iris.data[:-1,:], iris.target[:-1]"], "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": ["from sklearn.linear_model import LogisticRegression\n", "\n", "# our model - a multiclass regression\n", "logistic = LogisticRegression()\n", "\n", "# train on iris training set\n", "logistic.fit(X_train, y_train)\n", "\n", "X_test = iris.data[-1,:].reshape(1, -1)\n", "\n", "y_predict = logistic.predict(X_test)\n", "\n", "print('Predicted class %s, real class %s' % (\n", " y_predict, iris.target[-1]))\n", "\n", "print('Probabilities of membership in each class: %s' % \n", "      logistic.predict_proba(X_test))"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["QUESTION:\n", "* What would have been good to do before plunging right in to a logistic regression model?"], "cell_type": "markdown", "metadata": {}}, {"source": ["#### Some terms you will encouter as a Machine Learnest\n", "\n", "Term  | Definition\n", "------------- | -------------\n", "Training set  | set of data used to learn a model\n", "Test set  | set of data used to test a model\n", "Feature  | a variable (continuous, discrete, categorical, etc.) aka column\n", "Target  | Label (associated with dependent variable, what we predict)\n", "Learner  | Model or algorithm\n", "Fit, Train | learn a model with an ML algorithm using a training set\n", "Predict | w/ supervised learning, give a label to an unknown datum(data), w/ unsupervised decide if new data is weird, in which group, or what to do next with the new data\n", "Accuracy | percentage of correct predictions ((TP + TN) / total)\n", "Precision | percentage of correct positive predictions (TP / (FP + TP))\n", "Recall | percentage of positive cases caught (TP / (FN + TP))"], "cell_type": "markdown", "metadata": {}}, {"source": ["> PRO TIP: Are you a statitician?  Want to talk like a machine learning expert?  Here you go (from the friendly people at SAS ([here](http://www.sas.com/it_it/insights/analytics/machine-learning.html))): \n", "\n", "A Statistician Would Say  | A Machine Learnest Would Say\n", "------------- | -------------\n", "dependent variable  | target\n", "variable  | feature\n", "transformation  | feature creation\n"], "cell_type": "markdown", "metadata": {}}], "metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "pygments_lexer": "ipython3", "version": "3.4.3", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat_minor": 0}