{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Algorithms - Unsupervised Learning\n",
    "\n",
    ">  Reminder:  In machine learning, the problem of unsupervised learning is that of trying to find hidden structure in unlabeled data. Since the training set given to the learner is unlabeled, there is no error or reward signal to evaluate a potential solution. Basically, we are just finding a way to represent the data and get as much information from it that we can.\n",
    "\n",
    "HEY!  Remember PCA from above?  PCA is actually considered unsupervised learning.  We just put it up there because it's a good way to visualize data at the beginning of the ML process.\n",
    "\n",
    "Let's revisit it in a little more detail using the `iris` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn; seaborn.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA revisited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "# subset data to have only sepal width and petal length (cm) for simplification\n",
    "X = iris.data[:, 1:3]\n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "pca.fit(X)\n",
    "\n",
    "print(\"% of variance attributed to components: \"+ \\\n",
    "      ', '.join(['%.2f' % (x * 100) for x in pca.explained_variance_ratio_]))\n",
    "print('\\ncomponents and amount of variance explained by each feature:', pca.components_)\n",
    "print(pca.mean_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pca.explained_variance_` is like the magnitude of a components influence (amount of variance explained) and the `pca.components_` is like the direction vector for each feature in each component (directions of max variance in a feature in a component vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(X[:, 0], X[:, 1], 'o', alpha=0.5)\n",
    "means = pca.mean_\n",
    "\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    print(vector)\n",
    "    print(3*np.sqrt(length))\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    print(v)\n",
    "    plt.plot([means[0], v[0]+means[0]], [means[1], v[1]+means[1]], '-k', lw=3)\n",
    "\n",
    "# axis dims\n",
    "plt.xlim(0, max(X[:, 0])+3)\n",
    "plt.ylim(0, max(X[:, 1])+3)\n",
    "\n",
    "# labels\n",
    "plt.xlabel(iris.feature_names[1])\n",
    "plt.ylabel(iris.feature_names[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall, in the ML 101 module: unsupervised models have a `fit()`, `transform()` and/or `fit_transform()` in `sklearn`.\n",
    "\n",
    "\n",
    "If you want to both get a fit and new dataset with reduced dimensionality, which would you use below? (Fill in blank in code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get back to our 4D dataset\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "pca = PCA(n_components = 0.95) # keep 95% of variance\n",
    "X_trans = pca.___(X) # <- fill in the blank\n",
    "print(X.shape)\n",
    "print(X_trans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_trans[:, 0], X_trans[:, 1], c=iris.target, edgecolor='none', alpha=0.5,\n",
    "           cmap=plt.cm.get_cmap('spring', 10))\n",
    "plt.ylabel('Component 2')\n",
    "plt.xlabel('Component 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "KMeans finds cluster centers that are the mean of the points within them.  Likewise, a point is in a cluster because the cluster center is the closest cluster center for that point.\n",
    "\n",
    "KMeans employ the <i>Expectation-Maximization</i> algorithm which works as follows: \n",
    "\n",
    "1. Guess cluster centers\n",
    "* Assign points to nearest cluster\n",
    "* Set cluster centers to the mean of points\n",
    "* Repeat 1-3 until converged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "pca = PCA(n_components = 0.95) # keep 95% of variance\n",
    "X = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_clusters  = 3\n",
    "random_state = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _kmeans_step(frame=0, n_clusters=n_clusters):\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    labels = np.zeros(X.shape[0])\n",
    "    centers = rng.randn(n_clusters, 2)\n",
    "\n",
    "    nsteps = frame // 3\n",
    "\n",
    "    for i in range(nsteps + 1):\n",
    "        old_centers = centers\n",
    "        if i < nsteps or frame % 3 > 0:\n",
    "            dist = euclidean_distances(X, centers)\n",
    "            labels = dist.argmin(1)\n",
    "\n",
    "        if i < nsteps or frame % 3 > 1:\n",
    "            centers = np.array([X[labels == j].mean(0)\n",
    "                                for j in range(n_clusters)])\n",
    "            nans = np.isnan(centers)\n",
    "            centers[nans] = old_centers[nans]\n",
    "\n",
    "\n",
    "    # plot the data and cluster centers\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='rainbow',\n",
    "                vmin=0, vmax=n_clusters - 1);\n",
    "    plt.scatter(old_centers[:, 0], old_centers[:, 1], marker='o',\n",
    "                c=np.arange(n_clusters),\n",
    "                s=200, cmap='rainbow')\n",
    "    plt.scatter(old_centers[:, 0], old_centers[:, 1], marker='o',\n",
    "                c='black', s=50)\n",
    "\n",
    "    # plot new centers if third frame\n",
    "    if frame % 3 == 2:\n",
    "        for i in range(n_clusters):\n",
    "            plt.annotate('', centers[i], old_centers[i], \n",
    "                         arrowprops=dict(arrowstyle='->', linewidth=1))\n",
    "        plt.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                    c=np.arange(n_clusters),\n",
    "                    s=200, cmap='rainbow')\n",
    "        plt.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                    c='black', s=50)\n",
    "\n",
    "    plt.xlim(-4, 5)\n",
    "    plt.ylim(-2, 1.5)\n",
    "    plt.ylabel('PC 2')\n",
    "    plt.xlabel('PC 1')\n",
    "\n",
    "#     if frame % 3 == 1:\n",
    "#         plt.text(1.2, 4.5, \"1. Reassign points to nearest centroid\",\n",
    "#                  ha='right', va='top', size=8)\n",
    "#     elif frame % 3 == 2:\n",
    "#         plt.text(1.2, 4.5, \"2. Update centroids to cluster means\",\n",
    "#                  ha='right', va='top', size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# suppress future warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "min_clusters, max_clusters = 1, 6\n",
    "interact(_kmeans_step, frame=[0, 20],\n",
    "                    n_clusters=[min_clusters, max_clusters])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>Warning</b>! There is absolutely no guarantee of recovering a ground truth. First, choosing the right number of clusters is hard. Second, the algorithm is sensitive to initialization, and can fall into local minima, although scikit-learn employs several tricks to mitigate this issue.<br>  --Taken directly from sklearn docs\n",
    "\n",
    "<img src='imgs/pca1.png' alt=\"Original PCA with Labels\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Novelty detection aka anomaly detection\n",
    "QUICK QUESTION:\n",
    "What is the diffrence between outlier detection and anomaly detection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import rcParams, font_manager\n",
    "rcParams['figure.figsize'] = (14.0, 7.0)\n",
    "fprop = font_manager.FontProperties(size=14)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(-2, 9, 500), np.linspace(-2,9, 500))\n",
    "\n",
    "# Iris data\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "labels = iris.feature_names[1:3]\n",
    "X = X[:, 1:3]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "# make some outliers\n",
    "X_outliers = np.random.uniform(low=-2, high=9, size=(20, 2))\n",
    "\n",
    "# fit the model\n",
    "clf = svm.OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=1, random_state = 0)\n",
    "clf.fit(X_train)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred_test = clf.predict(X_test)\n",
    "y_pred_outliers = clf.predict(X_outliers)\n",
    "n_error_train = y_pred_train[y_pred_train == -1].size\n",
    "n_error_test = y_pred_test[y_pred_test == -1].size\n",
    "n_error_outliers = y_pred_outliers[y_pred_outliers == 1].size\n",
    "\n",
    "\n",
    "# plot the line, the points, and the nearest vectors to the plane\n",
    "Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.title(\"Novelty Detection\")\n",
    "plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.Blues_r)\n",
    "a = plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='red')\n",
    "plt.contourf(xx, yy, Z, levels=[0, Z.max()], colors='orange')\n",
    "\n",
    "b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c='white')\n",
    "b2 = plt.scatter(X_test[:, 0], X_test[:, 1], c='green')\n",
    "c = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c='red')\n",
    "plt.axis('tight')\n",
    "plt.xlim((-2, 9))\n",
    "plt.ylim((-2, 9))\n",
    "plt.ylabel(labels[1], fontsize = 14)\n",
    "plt.legend([a.collections[0], b1, b2, c],\n",
    "           [\"learned frontier\", \"training observations\",\n",
    "            \"new regular observations\", \"new abnormal observations\"],\n",
    "           loc=\"best\",\n",
    "           prop=fprop)\n",
    "plt.xlabel(\n",
    "    \"%s\\nerror train: %d/200 ; errors novel regular: %d/40 ; \"\n",
    "    \"errors novel abnormal: %d/10\"\n",
    "    % (labels[0], n_error_train, n_error_test, n_error_outliers), fontsize = 14)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "TRY changing the value of the parameters in the SVM classifier above especially `gamma`.  More information on `gamma` and support vector machine classifiers [here](http://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
