{"nbformat": 4, "cells": [{"source": ["> <b>ML TIP:  Ask sharp questions.</b><br>e.g. What type of flower is this (pictured below) closest to of the three given classes?\n", "\n", "(This links out to source)\n", "<a href=\"http://www.madlantern.com/photography/wild-iris/\"><img border=\"0\" alt=\"iris species\" src=\"imgs/iris-setosa.jpg\" width=\"400\" height=\"400\"></a>\n", "\n", "### Labels (species names/classes):\n", "(This links out to source)\n", "<a href=\"http://articles.concreteinteractive.com/machine-learning-a-new-tool-for-humanity/\"><img border=\"0\" alt=\"iris species\" src=\"imgs/irises.png\" width=\"500\" height=\"500\"></a>"], "cell_type": "markdown", "metadata": {}}, {"source": ["> NOTE: `sklearn` needs data/features (aka columns) in numpy ndarrays and the optional labels also as numpy ndarrays.\n", "\n", "> TIP: Commonly, machine learning algorithms will require your data to be standardized and preprocessed.  In `sklearn` the data must also take on a certain structure as well.</b>\n"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["print(type(iris.data))\n", "print(type(iris.target))"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["## Let's Dive In!"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["import seaborn as sb\n", "import pandas as pd\n", "import numpy as np\n", "\n", "#sb.set_context(\"notebook\", font_scale=2.5)\n", "%matplotlib inline"], "outputs": [], "metadata": {"collapsed": true}}, {"source": ["Features in the Iris dataset:\n", " \t\n", "0 sepal length in cm<br>\n", "1 sepal width in cm<br>\n", "2 petal length in cm<br>\n", "3 petal width in cm<br>\n", "\n", "Target classes to predict:\n", " \t\n", "0 Iris Setosa<br>\n", "1 Iris Versicolour<br>\n", "2 Iris Virginica<br>"], "cell_type": "markdown", "metadata": {}}, {"source": ["### Get to know the data - visualize and explore\n", "* Features (columns/measurements) come from this diagram (links out to source on kaggle):\n", "<a href=\"http://blog.kaggle.com/2015/04/22/scikit-learn-video-3-machine-learning-first-steps-with-the-iris-dataset/\"><img border=\"0\" alt=\"iris data features\" src=\"imgs/iris_petal_sepal.png\" width=\"200\" height=\"200\"></a>\n", "* Shape\n", "* Peek at data\n", "* Summaries"], "cell_type": "markdown", "metadata": {}}, {"source": ["<b>Shape and representation<b>"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["import pandas as pd\n", "from sklearn import datasets\n", "\n", "iris = datasets.load_iris()\n", "\n", "# How many data points (rows) x how many features (columns)\n", "print(iris.data.shape)\n", "print(iris.target.shape)\n", "\n", "# What python object represents\n", "print(type(iris.data))\n", "print(type(iris.target))"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["<b>Sneak a peek at data (a reminder of your `pandas` dataframe methods)<b>"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# convert to pandas df (adding real column names)\n", "iris.df = pd.DataFrame(iris.data, \n", "                       columns = ['Sepal length', 'Sepal width', 'Petal length', 'Petal width'])\n", "\n", "\n", "# first few rows\n", "iris.df.head()"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["<b>Describe the dataset with some summary statitsics<b>"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# summary stats\n", "iris.df.describe()"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["* We don't have to do much with the `iris` dataset.  It has no missing values.  It's already in numpy arrays and has the correct shape for `sklearn`.  However we could try <b>standardization</b> and/or <b>normalization</b>. (later, in the transforms section, we will show one hot encoding, a preprocessing step)"], "cell_type": "markdown", "metadata": {}}, {"source": ["### Preprocessing (Bonus Material)"], "cell_type": "markdown", "metadata": {}}, {"source": ["<p>What you might have to do before using a learner in `sklearn`:</p>\n", "1. Non-numerics transformed to numeric (tip: use applymap() method from `pandas`)\n", "* Fill in missing values\n", "* Standardization\n", "* Normalization\n", "* Encoding categorical features (e.g. one-hot encoding or dummy variables)\n", "\n", "<b>Features should end up in a numpy.ndarray (hence numeric) and labels in a list.</b>\n", "\n", "Data options:\n", "* Use pre-processed [datasets](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets) from scikit-learn\n", "* [Create your own](http://scikit-learn.org/stable/datasets/index.html#sample-generators)\n", "* Read from a file\n", "\n", "If you use your own data or \"real-world\" data you will likely have to do some data wrangling and need to leverage `pandas` for some data manipulation."], "cell_type": "markdown", "metadata": {"collapsed": true}}, {"source": ["#### Standardization - make our data look like a standard Gaussian distribution (commonly needed for `sklearn` learners)"], "cell_type": "markdown", "metadata": {}}, {"source": ["> FYI: you'll commonly see the data or feature set (ML word for data without it's labels) represented as a capital <b>X</b> and the targets or labels (if we have them) represented as a lowercase <b>y</b>.  This is because the data is a 2D array or list of lists and the targets are a 1D array or simple list."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# Standardization aka scaling\n", "from sklearn import preprocessing, datasets\n", "\n", "# make sure we have iris loaded\n", "iris = datasets.load_iris()\n", "\n", "X, y = iris.data, iris.target\n", "\n", "# scale it to a gaussian distribution\n", "X_scaled = preprocessing.scale(X)\n", "\n", "# how does it look now\n", "pd.DataFrame(X_scaled).head()"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": ["# let's just confirm our standardization worked (mean is 0 w/ unit variance)\n", "pd.DataFrame(X_scaled).describe()\n", "\n", "# also could:\n", "#print(X_scaled.mean(axis = 0))\n", "#print(X_scaled.std(axis = 0))"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["> PRO TIP: To save our standardization and reapply later (say to the test set or some new data), create a transformer object like so:\n", "```python\n", "scaler = preprocessing.StandardScaler().fit(X_train)\n", "# apply to a new dataset (e.g. test set):\n", "scaler.transform(X_test)\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["#### Normalization - scaling samples <i>individually</i> to have unit norm\n", "* This type of scaling is really important if doing some downstream transformations and learning (see sklearn docs [here](http://scikit-learn.org/stable/modules/preprocessing.html#normalization) for more) where similarity of pairs of samples is examined\n", "* A basic intro to normalization and the unit vector can be found [here](http://freetext.org/Introduction_to_Linear_Algebra/Basic_Vector_Operations/Normalization/)"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# Standardization aka scaling\n", "from sklearn import preprocessing, datasets\n", "\n", "# make sure we have iris loaded\n", "iris = datasets.load_iris()\n", "\n", "X, y = iris.data, iris.target\n", "\n", "# scale it to a gaussian distribution\n", "X_norm = preprocessing.normalize(X, norm='l1')\n", "\n", "# how does it look now\n", "pd.DataFrame(X_norm).tail()"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": ["# let's just confirm our standardization worked (mean is 0 w/ unit variance)\n", "pd.DataFrame(X_norm).describe()\n", "\n", "# cumulative sum of normalized and original data:\n", "#print(pd.DataFrame(X_norm.cumsum().reshape(X.shape)).tail())\n", "#print(pd.DataFrame(X).cumsum().tail())\n", "\n", "# unit norm (convert to unit vectors) - all row sums should be 1 now\n", "X_norm.sum(axis = 1)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["> PRO TIP: To save our normalization (like standardization above) and reapply later (say to the test set or some new data), create a transformer object like so:\n", "```python\n", "normalizer = preprocessing.Normalizer().fit(X_train)\n", "# apply to a new dataset (e.g. test set):\n", "normalizer.transform(X_test) \n", "```"], "cell_type": "markdown", "metadata": {}}], "metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "pygments_lexer": "ipython3", "version": "3.4.3", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat_minor": 0}